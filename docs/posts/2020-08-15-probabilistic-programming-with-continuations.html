<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Jules Hedges - Probabilistic programming with continuations</title>
        <link rel="stylesheet" href="../css/default.css" />
    </head>
    <body>
        <header>
            <div class="logo">
                <a href="../">Jules Hedges</a>
            </div>
            <nav>
                <a href="../">Home</a>
                <a href="../papers.html">Papers</a>
                <a href="../links.html">Links</a>
                <a href="../archive.html">Archive</a>
            </nav>
        </header>

        <main role="main">
            <h1>Probabilistic programming with continuations</h1>
            <article>
    <section class="header">
        Posted on August 15, 2020
        
    </section>
    <section>
        <p>In this post I’ll explain something folkloric: that you can pretend that the continuation monad is a probability monad, and do probabilistic programming in it. Unlike more obvious representations of probability like the one in <a href="https://hackage.haskell.org/package/probability-0.2.7/docs/Numeric-Probability-Distribution.html"><code>Numeric.Probability.Distribution</code></a> via lists, this way works equally well for continuous as for discrete distributions (as long as you don’t mind numerical integration). The post is a literate Haskell program, which is an expanded version of <a href="https://github.com/jules-hedges/synthetic-probability/blob/master/src/SyntheticProbability.hs">this repository</a>. It’s a sort of sequel to my very first blog post, <a href="../posts/2016-09-22-abusing-continuation-monad.html">Abusing the continuation monad</a>.</p>
<p>I mentally call this idea “synthetic measure theory” or sometimes “synthetic probability”, although as far as I know it is not related to various Google hits for those terms such as <a href="https://ncatlab.org/nlab/show/synthetic+probability+theory">this</a>, or <a href="https://arxiv.org/abs/1908.07021">this</a>. (But one of the hits is <a href="https://www.cs.au.dk/~spitters/ProbProg.pdf">this paper</a>, which is probably related.)</p>
<p>Since this post is literate Haskell, we’d better get some imports out of the way. Obviously,</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="dt">Control.Monad.Cont</span></span></code></pre></div>
<p>Later, to avoid the headache of hacking up a bad trapezium method by hand, we’ll also need</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="dt">Numeric.Tools.Integration</span></span></code></pre></div>
<p>The mathematical fact behind “synthetic measure theory” is called the <a href="https://en.wikipedia.org/wiki/Riesz%E2%80%93Markov%E2%80%93Kakutani_representation_theorem">Riesz representation theorem</a>, which says roughly that measures are equivalent to linear functionals. Every measure <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math> on a space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> induces an integral operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>↦</mo><msub><mo>∫</mo><mi>X</mi></msub><mi>k</mi><mspace width="0.167em"></mspace><mi mathvariant="normal">d</mi><mi>μ</mi></mrow><annotation encoding="application/x-tex">k \mapsto \int_X k \, \mathrm{d} \mu</annotation></semantics></math>. This is a functional on the space of continuous functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">X \to \mathbb{R}</annotation></semantics></math>, and moreover it is a linear functional, because integration is linear. The Riesz representation theorem (really, any of the family of related theorems) is the converse of this: it says that every linear operator that <em>looks like</em> the integration operator for some measure actually is, uniquely. So very loosely, in a sense, measure theory and functional analysis are equivalent.</p>
<p>This probably sounds as clear as mud, because measure theory is very esoteric, but I’ll illustrate it by example through the rest of this post.</p>
<p>In Haskell, the type of the integration operator is <code>(X -&gt; Double) -&gt; Double</code>, also known as <code>Cont Double X</code>. For the Riesz theorem to work it is crucial that the second <code>-&gt;</code> refers only to functions that are linear (in the sense of linear algebra). We can’t do that in Haskell (or at least if we can then I don’t know how to do it<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>), so we have a lossy representation and must rely on the user not to write anything that isn’t linear.</p>
<p>So let’s define a probability monad like this:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">Prob</span> <span class="ot">=</span> <span class="dt">Cont</span> <span class="dt">Double</span></span></code></pre></div>
<p>The general idea, with the scary words removed, is that we represent a probability distribution using the operator that does integration <em>weighted by</em> that probability distribution. Here, for example, is the operator that represents an unbiased coin flip:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Coin</span> <span class="ot">=</span> <span class="dt">Heads</span> <span class="op">|</span> <span class="dt">Tails</span> <span class="kw">deriving</span> (<span class="dt">Eq</span>, <span class="dt">Show</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">flip</span><span class="ot"> ::</span> <span class="dt">Prob</span> <span class="dt">Coin</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">flip</span> <span class="ot">=</span> cont <span class="op">$</span> \k <span class="ot">-&gt;</span> (k <span class="dt">Heads</span> <span class="op">+</span> k <span class="dt">Tails</span>) <span class="op">/</span> <span class="dv">2</span></span></code></pre></div>
<p>More generally, we can represent the discrete uniform distribution on any finite set of points:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ot">uniformDiscrete ::</span> [a] <span class="ot">-&gt;</span> <span class="dt">Prob</span> a</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>uniformDiscrete xs <span class="ot">=</span> cont <span class="op">$</span> \k <span class="ot">-&gt;</span> <span class="fu">sum</span> (<span class="fu">map</span> k xs) <span class="op">/</span> <span class="fu">fromIntegral</span> (<span class="fu">length</span> xs)</span></code></pre></div>
<p>(where <code>fromIntegral</code> doesn’t actually do anything, just wrangling different number types). We could if we wanted also <em>abuse</em> <code>uniformDiscrete</code> to represent non-uniform distributions by giving it a list with repeated elements.</p>
<p>An <em>event</em> is a predicate, i.e. a function into booleans. In order to get the probability of an event, we integrate its “indicator function”, i.e. the equivalent function into <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ot">probability ::</span> <span class="dt">Prob</span> a <span class="ot">-&gt;</span> (a <span class="ot">-&gt;</span> <span class="dt">Bool</span>) <span class="ot">-&gt;</span> <span class="dt">Double</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>probability integrate p <span class="ot">=</span> runCont integrate <span class="op">$</span> \x <span class="ot">-&gt;</span> <span class="kw">if</span> p x <span class="kw">then</span> <span class="dv">1</span> <span class="kw">else</span> <span class="dv">0</span></span></code></pre></div>
<p>With this, we can run <code>probability coin (== Heads)</code> and get the answer <code>0.5</code>. For dice rather than coins we can run <code>probability (uniformDiscrete [1..6]) even</code>, and also get the answer <code>0.5</code>.</p>
<p>If we have a distribution on <code>Double</code>s we can also easily get its expectation, by integrating the identity function:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ot">expectation ::</span> <span class="dt">Prob</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Double</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>expectation integrate <span class="ot">=</span> runCont integrate <span class="fu">id</span></span></code></pre></div>
<p>For example, to get the expected value of a dice roll we can run <code>expectation (uniformDiscrete [1.0 .. 6.0])</code> and be correctly told <code>3.5</code>. (It’s best not to think too hard about how the Haskell Prelude installs <code>instance Enum Double</code> by default.)</p>
<p>The other mathematical fact behind the whole thing is that integration defines a monad morphism from the probability monad (any of them, including the ones that can’t be expressed in Haskell) to the continuation monad. This fact packages up a whole lot of information saying that the monad structure of the continuation monad <em>does the right thing</em> on (things that represent) probability distributions. What that means in practice is that do-notation in the continuation monad can be used as a probabilistic programming language:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ot">twoDice ::</span> <span class="dt">Prob</span> <span class="dt">Int</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>twoDice <span class="ot">=</span> <span class="kw">do</span> roll1 <span class="ot">&lt;-</span> uniformDiscrete [<span class="dv">1</span><span class="op">..</span><span class="dv">6</span>]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>             roll2 <span class="ot">&lt;-</span> uniformDiscrete [<span class="dv">1</span><span class="op">..</span><span class="dv">6</span>]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>             <span class="fu">return</span> (roll1 <span class="op">+</span> roll2)</span></code></pre></div>
<p>Then we can ask <code>probability twoDice (== 6)</code> and be told <code>0.13888888888888887</code>, also known as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi>/</mi><mn>36</mn></mrow><annotation encoding="application/x-tex">5/36</annotation></semantics></math>. We can also run <code>expectation (fmap fromIntegral twoDice)</code> and be told <code>7.0</code>.</p>
<p>Where this method really comes into its own is for representing continuous probability distributions. Here is the uniform distribution on the unit interval:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ot">uniformUnitInterval ::</span> <span class="dt">Prob</span> <span class="dt">Double</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>uniformUnitInterval <span class="ot">=</span> cont <span class="op">$</span> \k <span class="ot">-&gt;</span> quadBestEst (quadTrapezoid params (<span class="dv">0</span>, <span class="dv">1</span>) k)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span> params <span class="ot">=</span> <span class="dt">QuadParam</span> {quadPrecision <span class="ot">=</span> <span class="fl">0.00001</span>, quadMaxIter <span class="ot">=</span> <span class="dv">30</span>}</span></code></pre></div>
<p>Although under the hood this now uses numerical integration, from the outside we can interact with it in exactly the same way as for discrete distributions. For example we can run <code>probability uniformUnitInterval (&lt;= 0.5)</code> and get back <code>0.5000038146972656</code>, which is close enough to a half for government work. Also <code>expectation uniformUnitInterval</code> gives <code>0.5</code> on the nose.</p>
<p>For a slightly less trivial example, I ran <code>probability (do {x &lt;- uniformUnitInterval; y &lt;- uniformUnitInterval; return (x*y)}) (&lt;= 0.5)</code> for the product of two independent uniformly distribution random variables. For me this took several seconds before getting the answer <code>0.8465757742524147</code>.</p>
<p>I wanted to demo a normal distribution for the grand finale, but I couldn’t find an existing density function for normal distributions written in Haskell, so I decided it would be too much effort.</p>
<p>Doing probability this way opens the Pandora’s box of numerical integration, of course. It occurs to me that if we used Monte Carlo integration rather than trapezium, this would start to blur the boundary between probabilistic programming and merely programming with a random number generator.</p>
<p>One last thing. Doing probabilistic programming in the continuation monad invites the question, <em>what does call/cc do in probability?!?</em> Unfortunately, after running an example I found that call/cc does not preserve linearity, so it is not a well-defined operation in probability. I’ll give the full demonstration, partly just as one more example of doing probability theory in this style.</p>
<p>The relevant type of call/cc here is <code>((a -&gt; Prob b) -&gt; Prob a) -&gt; Prob a</code>. A nontrivial example of something with the input type is Bayesian updating for arbitrary conditional distributions, given a fixed prior and observation. Fix</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">X</span> <span class="ot">=</span> <span class="dt">X1</span> <span class="op">|</span> <span class="dt">X2</span> <span class="kw">deriving</span> (<span class="dt">Eq</span>, <span class="dt">Show</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> <span class="dt">Y</span> <span class="ot">=</span> <span class="dt">Y1</span> <span class="op">|</span> <span class="dt">Y2</span> <span class="kw">deriving</span> (<span class="dt">Eq</span>, <span class="dt">Show</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> uniformDiscrete [<span class="dt">X1</span>, <span class="dt">X2</span>]</span></code></pre></div>
<p>Supposing we observe the output <code>Y1</code>, for an arbitrary conditional distribution <code>f :: X1 -&gt; Prob X2</code> the posterior probabilities of <code>X1</code> and <code>X2</code> are respectively given by <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem#Statement_of_theorem">Bayes’ law</a>:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>px1 f <span class="ot">=</span> probability (f <span class="dt">X1</span>) (<span class="op">==</span> <span class="dt">Y1</span>) <span class="op">*</span> probability prior (<span class="op">==</span> <span class="dt">X1</span>) <span class="op">/</span> probability (prior <span class="op">&gt;&gt;=</span> f) (<span class="op">==</span> <span class="dt">Y1</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>px2 f <span class="ot">=</span> probability (f <span class="dt">X2</span>) (<span class="op">==</span> <span class="dt">Y1</span>) <span class="op">*</span> probability prior (<span class="op">==</span> <span class="dt">X2</span>) <span class="op">/</span> probability (prior <span class="op">&gt;&gt;=</span> f) (<span class="op">==</span> <span class="dt">Y1</span>)</span></code></pre></div>
<p>For example, suppose we define the conditional distribution</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ot">f ::</span> <span class="dt">X</span> <span class="ot">-&gt;</span> <span class="dt">Prob</span> <span class="dt">X2</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>f <span class="dt">X1</span> <span class="ot">=</span> cont <span class="op">$</span> \k <span class="ot">-&gt;</span> (<span class="dv">2</span> <span class="op">*</span> k <span class="dt">Y1</span> <span class="op">+</span> k <span class="dt">Y2</span>) <span class="op">/</span> <span class="dv">3</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>f <span class="dt">X2</span> <span class="ot">=</span> cont <span class="op">$</span> \k <span class="ot">-&gt;</span> (k <span class="dt">Y1</span> <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> k <span class="dt">Y2</span>) <span class="op">/</span> <span class="dv">3</span></span></code></pre></div>
<p>which represents the stochastic channel with scattering matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn><mi>/</mi><mn>3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn><mi>/</mi><mn>3</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn><mi>/</mi><mn>3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>2</mn><mi>/</mi><mn>3</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{pmatrix} 2/3 &amp; 1/3 \\ 1/3 &amp; 2/3 \end{pmatrix}</annotation></semantics></math>. Then <code>px1 f</code> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">2/3</annotation></semantics></math> and <code>px2 f</code> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">1/3</annotation></semantics></math>. (There might be a neater way to do Bayesian inversion in terms of integration operators, but I can’t think of it right now.)</p>
<p>The function bayes has type <code>(X1 -&gt; Prob X2) -&gt; Prob X1</code>, so <code>callCC bayes</code> has type <code>Prob X1</code>. Unfortunately it isn’t linear: the “total probability” <code>probability (callCC bayes) (const True)</code> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>, but also <code>probability (callCC bayes) (== X1)</code> and <code>probability (callCC bayes) (== X2)</code> are <em>both</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>. So disappointingly, there’s no call/cc in probability.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Editor’s note: It might be interesting to try this with <a href="https://ucsd-progsys.github.io/liquidhaskell/">Liquid Haskell</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Editor’s note: This post received a reply from Reuben Cohn-Gordon:</p>
<blockquote>
<p><a href="https://hackage.haskell.org/package/monad-bayes-1.1.0/docs/Control-Monad-Bayes-Integrator.html">https://hackage.haskell.org/package/monad-bayes-1.1.0/docs/Control-Monad-Bayes-Integrator.html</a> now implements basically what you describe above, and lets you work with the normal distribution (and most others). Some examples here: <a href="http://web.archive.org/web/20221118175501/https://monad-bayes-site.netlify.app/introduction">https://monad-bayes-site.netlify.app/introduction</a></p>
</blockquote>
<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></li>
</ol>
</section>
    </section>
</article>

        </main>

        <footer>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </footer>
    </body>
</html>
